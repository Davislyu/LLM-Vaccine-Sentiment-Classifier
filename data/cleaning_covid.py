# -*- coding: utf-8 -*-
"""cleaning_Covid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eq70EDV5EtuzoNGa7LSDljcG13Jbdr2G
"""







import pandas as pd

df = pd.read_excel("FilteredData2.xlsx")


df

import matplotlib.pyplot as plt

# Plot bar chart of label distribution
df['LABEL'].value_counts().plot(kind='bar', color='skyblue')
plt.xlabel('Label')
plt.ylabel('Frequency')
plt.title('Label Distribution')
plt.show()

# prompt: Show me the COUNT of the values 1,2,3 from the LABEL column

df['LABEL'].value_counts()

# Remove duplicate rows based on all columns
df.drop_duplicates(inplace=True)

# Display the shape of the DataFrame
print("Shape after removing duplicates:", df.shape)
df

# Remove rows with duplicate "Id_b36"
df.drop_duplicates(subset=["Id_b36"], inplace=True)

# Display the shape of the DataFrame
print("Shape after removing duplicates based on 'Id_b36':", df.shape)

# Filter rows based on specific conditions
# For example, filtering rows where text is not empty
df = df[df["text"].notnull()]

# Display the shape of the DataFrame
print("Shape after filtering rows with non-null text:", df.shape)

import pandas as pd



# Calculate the number of symbols in each text
symbol_counts = df["text"].str.count(r"[!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]")

# Calculate the total number of characters in each text
total_chars = df["text"].str.len()

# Calculate the percentage of symbols in each text
symbol_percentage = (symbol_counts / total_chars) * 100

# Filter rows based on symbol percentage
df = df[symbol_percentage <= 80]

df

import pandas as pd
import difflib

# Load the Excel file into a DataFrame

# Define a function to calculate text similarity
def text_similarity(text1, text2):
    return difflib.SequenceMatcher(None, text1, text2).ratio()

# Group the DataFrame by author_id_b36
grouped = df.groupby("author_id_b36")

# Initialize a list to store indices of rows to keep
indices_to_keep = []

# Iterate through each group
for group_name, group_df in grouped:
    # Initialize variables to store indices of rows to remove within the group
    indices_to_remove = set()

    # Iterate through each row in the group
    for index, row in group_df.iterrows():
        # Compare the text of the current row with texts of other rows in the group
        for other_index, other_row in group_df.iterrows():
            if index != other_index and other_index not in indices_to_remove:
                similarity_ratio = text_similarity(row["text"], other_row["text"])
                # If the similarity ratio is above 90%, mark the row with the shorter text for removal
                if similarity_ratio > 0.95:
                    if len(row["text"]) < len(other_row["text"]):
                        indices_to_remove.add(index)
                    else:
                        indices_to_remove.add(other_index)

    # Add indices of rows to keep within the group
    indices_to_keep.extend(set(group_df.index) - indices_to_remove)

# Filter the DataFrame to keep only the rows with indices in indices_to_keep
df = df.loc[indices_to_keep]

# Reset the index of the filtered DataFrame
df.reset_index(drop=True, inplace=True)

# Display the first few rows of the filtered DataFrame
df

import string

# Define a function to calculate the percentage of symbols in a text
def symbol_percentage(text):
    total_symbols = sum(1 for char in text if char in string.punctuation)
    total_chars = len(text)
    return (total_symbols / total_chars) * 100 if total_chars != 0 else 0

# Filter rows based on symbol percentage
df = df[df["text"].apply(lambda x: symbol_percentage(x) <= 20)]

# Reset the index of the filtered DataFrame
df.reset_index(drop=True, inplace=True)

# Display the first few rows of the filtered DataFrame
df

# Define a function to calculate the percentage of numeric characters in a text
def numeric_percentage(text):
    total_chars = len(text)
    if total_chars == 0:
        return 0
    numeric_chars = sum(1 for char in text if char.isdigit())
    return (numeric_chars / total_chars) * 100

# Filter rows where the percentage of numeric characters exceeds 20%
df = df[df['text'].apply(lambda x: numeric_percentage(x) <= 20)]


df

# Remove rows with long text
max_text_length = 800
df = df[df['text'].str.len() <= max_text_length]
df

import re

# Remove rows containing HTML tags
df = df[~df['text'].str.contains(r'<.*?>')]
df

df.to_excel("333.xlsx", index=False)

import re

# Remove non-alphanumeric characters except for spaces, URLs, and retweets
df['text'] = df['text'].apply(lambda x: re.sub(r'[^\w\s]|(http\S+)|(RT)', '', x))

# Convert to lowercase
df['text'] = df['text'].str.lower()

# Remove unnecessary spaces
df['text'] = df['text'].str.strip()

df

df.drop_duplicates(subset=['text'], inplace=True)



df

import re

# Define a function to replace "_" with space sign
def replace_underscore(text):
    # Define regular expression pattern to match "_" adjacent to a word or within a word
    pattern = r'\b_\b|\b_\w+|\w+_\b|\w+_\w+'
    # Replace "_" with space sign
    text = re.sub(pattern, ' ', text)
    return text

# Apply the function to the "text" column of your DataFrame
df['text'] = df['text'].apply(replace_underscore)

# Display the DataFrame
df

import re

# Define a function to insert space between words and numbers
def insert_space(text):
    # Define regular expression pattern to match words adjacent to numbers without spaces
    pattern = r'(\b[a-zA-Z]+)(\d+\b)'
    # Replace the pattern with a space between the word and number
    text = re.sub(pattern, r'\1 \2', text)
    return text

# Apply the function to the "text" column of your DataFrame
df['text'] = df['text'].apply(insert_space)

# Display the DataFrame
df

import re

# Define a function to remove words containing specific characters
def remove_words_with_characters(text):
    # Define regular expression pattern to match words containing specific characters
    pattern = r'\b\w*[ðÿêñøè]+\w*\b'
    # Remove words matching the pattern
    text = re.sub(pattern, '', text)
    return text

# Apply the function to the "text" column of your DataFrame
df['text'] = df['text'].apply(remove_words_with_characters)

# Display the DataFrame
df

import re

# Define a pattern to match non-English characters
non_english_pattern = r'\b(?:ç|ñ|ð|ÿ|â|ê|ø|è|â|ç|ג)\w*\b'

# Function to remove words containing non-English characters
def remove_non_english_words(text):
    # Use regular expression to replace non-English words with empty strings
    cleaned_text = re.sub(non_english_pattern, '', text)
    return cleaned_text

# Apply the function to the "text" column of your DataFrame
df['text'] = df['text'].apply(remove_non_english_words)

# Display the DataFrame
df



!pip install langdetect

from langdetect import detect

# Function to check if text is in English
def is_english(text):
    try:
        return detect(text) == 'en'
    except:
        return False

# Filter out non-English rows
english_texts = df['text'].apply(is_english)
df = df[english_texts]

# Update the original DataFrame
# df = df[english_texts].copy() # Uncomment this line if you want to create a new DataFrame instead of modifying the original one

# Optionally, reset the index if needed
df.reset_index(drop=True, inplace=True)
current_df = df

print(current_df)

current_df.to_excel("last_clean_team_new6900.xlsx", index=False)



import matplotlib.pyplot as plt

# Plot histogram of text lengths
plt.hist(current_df['text'].apply(lambda x: len(x)), bins=30, color='skyblue')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.title('Histogram of Text Lengths')
plt.show()

import matplotlib.pyplot as plt

# Plot bar chart of label distribution
current_df['LABEL'].value_counts().plot(kind='bar', color='skyblue')
plt.xlabel('Label')
plt.ylabel('Frequency')
plt.title('Label Distribution')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Define a dictionary to map numerical labels to their corresponding names
label_names = {1: 'neutral', 2: 'misinformation', 3: 'counter-misinformation'}

# Replace numerical labels with names
current_df['LABEL'] = current_df['LABEL'].replace(label_names)

# Plot bar chart of label distribution
current_df['LABEL'].value_counts().plot(kind='bar', color='skyblue')
plt.xlabel('Label')
plt.ylabel('Frequency')
plt.title('Label Distribution')
plt.show()

# prompt: Show me the COUNT of the values 1,2,3 from the LABEL column

current_df['LABEL'].value_counts()

from wordcloud import WordCloud

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(current_df['text']))

# Display word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Most Frequent Words')
plt.show()

from collections import Counter

word_counts = Counter(' '.join(current_df['text']).split())
top_n_words = word_counts.most_common(10)  # Change 10 to any number of top words you want to visualize

plt.bar(*zip(*top_n_words))
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Top Words')
plt.xticks(rotation=45)
plt.show()



current_df

import pandas as pd

# Read the DataFrame with the 'CreatedAt' column
current_df = pd.read_excel('last_clean_team_new6900.xlsx')

# Convert 'CreatedAt' column to datetime format, handling errors
current_df['CreatedAt'] = pd.to_datetime(current_df['CreatedAt'], errors='coerce')

# Drop rows with NaT values in 'CreatedAt' column
current_df = current_df.dropna(subset=['CreatedAt'])

# Extract the date component and set it as the index
current_df['Date'] = current_df['CreatedAt'].dt.date

# Group by date and count the number of tweets
tweet_counts = current_df.groupby('Date').size()

# Plot the tweet counts over time
tweet_counts.plot(kind='line', figsize=(10, 6), title='Number of Tweets Over Time')

# Filter the DataFrame for each label
label_1 = current_df[current_df['LABEL'] == 1]
label_2 = current_df[current_df['LABEL'] == 2]
label_3 = current_df[current_df['LABEL'] == 3]

# Calculate descriptive statistics for each label
stats_label_1 = label_1.describe()
stats_label_2 = label_2.describe()
stats_label_3 = label_3.describe()

import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns

# # Read the DataFrame
# current_df = pd.read_excel('last_clean_team_new.xlsx')

# # Set up seaborn style for better visualization
# sns.set(style="whitegrid")

# # Create subplots for each label
# fig, axes = plt.subplots(3, 1, figsize=(12, 18))

# # Plot graphs for each label
# for i, label in enumerate([1, 2, 3]):
#     sns.histplot(current_df[current_df['LABEL'] == label], ax=axes[i], kde=True)
#     axes[i].set_title(f'Distribution of Features for Label {label}')
#     axes[i].set_xlabel('Value')
#     axes[i].set_ylabel('Frequency')

# plt.tight_layout()
# plt.show()

# import pandas as pd

# # Load the CSV file
# df = pd.read_csv('ClassifiedTweets.csv')

# # Display the first few rows of the DataFrame
# df

# # Count unique values in the "classification" column
# classification_counts = df['classification'].value_counts()

# print(classification_counts)

# # Display all unique values in the "classification" column
# unique_classifications = df['classification'].unique()
# print(unique_classifications)

# # Replace values in the "classification" column
# df['classification'] = df['classification'].replace({
#     'Irrelevant': "1",
#     'missinformation': "2",
#     'counter-missinformation': "3"
# })

# df

# # Drop rows with NaN values in the "classification" column
# df.dropna(subset=['classification'], inplace=True)

# # Save the modified DataFrame to a new CSV file
# df.to_csv('modified_classified_tweets.csv', index=False)

df

# import pandas as pd
# import matplotlib.pyplot as plt

# # Assuming 'CreatedAt' is already in datetime format and 'LABEL' is already converted to 'neutral', 'misinformation', 'counter-misinformation'
# misinformation_df = current_df[current_df['LABEL'] == 'misinformation']

# # Group by date and count the number of tweets
# misinformation_tweet_counts = misinformation_df.groupby(misinformation_df['CreatedAt'].dt.date).size()

# # Plot the tweet counts over time
# plt.figure(figsize=(10, 6))
# misinformation_tweet_counts.plot(kind='line', title='Number of Misinformation Tweets Over Time')
# plt.xlabel('Date')
# plt.ylabel('Number of Tweets')
# plt.grid(True)
# plt.show()

# import seaborn as sns

# plt.figure(figsize=(10, 6))
# sns.countplot(data=current_df, x='LABEL', palette='viridis')
# plt.xlabel('Label')
# plt.ylabel('Count')
# plt.title('Distribution of Tweets by Classification')
# plt.show()

!pip install wordcloud

# from wordcloud import WordCloud  # Correct import

# # Combine all text from misinformation tweets
# misinformation_text = ' '.join(misinformation_df['text'])

# # Generate word cloud
# wordcloud = WordCloud(width=800, height=400, background_color='white').generate(misinformation_text)

# # Plot the word cloud
# plt.figure(figsize=(10, 6))
# plt.imshow(wordcloud, interpolation='bilinear')
# plt.axis('off')
# plt.title('Common Words in Misinformation Tweets')
# plt.show()

plt.figure(figsize=(15, 10))

# Likes
plt.subplot(3, 1, 1)
sns.boxplot(data=current_df, x='LABEL', y='mLikes', palette='viridis')
plt.xlabel('')
plt.ylabel('Likes')
plt.title('Distribution of Likes by Classification')

# Retweets
plt.subplot(3, 1, 2)
sns.boxplot(data=current_df, x='LABEL', y='mRetweets', palette='viridis')
plt.xlabel('')
plt.ylabel('Retweets')
plt.title('Distribution of Retweets by Classification')

# Replies
plt.subplot(3, 1, 3)
sns.boxplot(data=current_df, x='LABEL', y='mReplies', palette='viridis')
plt.xlabel('Classification')
plt.ylabel('Replies')
plt.title('Distribution of Replies by Classification')

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(data=current_df, x='LABEL', y='mHasURL', palette='viridis')
plt.xlabel('Classification')
plt.ylabel('Proportion with URLs')
plt.title('Proportion of Tweets with URLs by Classification')
plt.show()

top_sources = current_df['author_id_b36'].value_counts().head(10)

plt.figure(figsize=(10, 6))
top_sources.plot(kind='bar', color='skyblue')
plt.xlabel('Author ID')
plt.ylabel('Number of Misinformation Tweets')
plt.title('Top 10 Sources of Misinformation Tweets')
plt.show()

# current_df['Hour'] = current_df['CreatedAt'].dt.hour

# plt.figure(figsize=(10, 6))
# sns.histplot(data=misinformation_df, x='Hour', bins=24, kde=True, color='skyblue')
# plt.xlabel('Hour of Day')
# plt.ylabel('Number of Misinformation Tweets')
# plt.title('Distribution of Misinformation Tweets by Hour of Day')
# plt.grid(True)
# plt.show()

# from sklearn.feature_extraction.text import CountVectorizer

# def plot_word_freq(label):
#     vectorizer = CountVectorizer(stop_words='english', max_features=20)
#     label_df = current_df[current_df['LABEL'] == label]
#     word_counts = vectorizer.fit_transform(label_df['text'])
#     word_freq = dict(zip(vectorizer.get_feature_names_out(), word_counts.sum(axis=0).A1))
#     word_freq = pd.Series(word_freq).sort_values(ascending=False)

#     plt.figure(figsize=(10, 6))
#     word_freq.plot(kind='bar', color='skyblue')
#     plt.title(f'Top Words in {label.capitalize()} Tweets')
#     plt.xlabel('Words')
#     plt.ylabel('Frequency')
#     plt.show()

# plot_word_freq('neutral')
# plot_word_freq('misinformation')
# plot_word_freq('counter-misinformation')

# from textblob import TextBlob

# current_df['Sentiment'] = current_df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)

# plt.figure(figsize=(15, 5))

# # Sentiment Distribution
# plt.subplot(1, 3, 1)
# sns.histplot(data=current_df[current_df['LABEL'] == 'neutral'], x='Sentiment', kde=True, color='blue')
# plt.title('Sentiment Distribution for Neutral Tweets')

# plt.subplot(1, 3, 2)
# sns.histplot(data=current_df[current_df['LABEL'] == 'misinformation'], x='Sentiment', kde=True, color='red')
# plt.title('Sentiment Distribution for Misinformation Tweets')

# plt.subplot(1, 3, 3)
# sns.histplot(data=current_df[current_df['LABEL'] == 'counter-misinformation'], x='Sentiment', kde=True, color='green')
# plt.title('Sentiment Distribution for Counter-Misinformation Tweets')

# plt.tight_layout()
# plt.show()

# import geopandas as gpd

# # Assuming we have latitude and longitude columns
# gdf = gpd.GeoDataFrame(current_df, geometry=gpd.points_from_xy(current_df.longitude, current_df.latitude))

# world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))

# fig, ax = plt.subplots(figsize=(15, 10))
# world.plot(ax=ax, color='lightgrey')
# gdf.plot(ax=ax, markersize=1, color='red', alpha=0.5)
# plt.title('Geographical Distribution of Tweets')
# plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming your DataFrame is named current_df and it has the 'LABEL' and 'author_id_b36' columns
# Read the DataFrame
# current_df = pd.read_csv('path_to_your_file.csv')

# Group by user and label, then count the tweets
user_label_counts = current_df.groupby(['author_id_b36', 'LABEL']).size().unstack(fill_value=0)

# Calculate the total tweet counts for each user
user_label_counts['total'] = user_label_counts.sum(axis=1)

# Sort users by total tweet counts and get the top 10 users
top_users = user_label_counts.sort_values(by='total', ascending=False).head(10)

# Drop the 'total' column as it's not needed for plotting
top_users = top_users.drop(columns=['total'])

# Plot the data using a stacked bar chart
top_users.plot(kind='bar', stacked=True, figsize=(12, 8), color=['skyblue', 'salmon', 'lightgreen'])
plt.xlabel('User')
plt.ylabel('Number of Tweets')
plt.title('Top 10 Users by Number of Tweets with Breakdown by Label')
plt.legend(title='Label', labels=['Neutral', 'Misinformation', 'Counter-Misinformation'])
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y')

plt.show()

current_df['Hour'] = current_df['CreatedAt'].dt.hour

plt.figure(figsize=(10, 6))
sns.histplot(data=current_df, x='Hour', bins=24, kde=True, color='skyblue')
plt.xlabel('Hour of Day')
plt.ylabel('Number of Misinformation Tweets')
plt.title('Distribution of Misinformation Tweets by Hour of Day')
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Ensure 'CreatedAt' is in datetime format
current_df['CreatedAt'] = pd.to_datetime(current_df['CreatedAt'], errors='coerce')

# Group by date and count the number of tweets
tweet_counts = current_df.groupby(current_df['CreatedAt'].dt.date).size()

# Plot the tweet counts over time
plt.figure(figsize=(12, 6))
tweet_counts.plot(kind='line')
plt.xlabel('Date')
plt.ylabel('Number of Tweets')
plt.title('Tweet Activity Over Time')
plt.grid(True)
plt.show()

# Group by 'author_id_b36' and count the number of tweets per user
top_users = current_df['author_id_b36'].value_counts().head(10)

# Plot the top 10 users
plt.figure(figsize=(12, 6))
top_users.plot(kind='bar', color='skyblue')
plt.xlabel('User')
plt.ylabel('Number of Tweets')
plt.title('Top 10 Users by Number of Tweets')
plt.show()

current_df['Hour'] = current_df['CreatedAt'].dt.hour

# Group by hour and calculate the average retweets and likes
hourly_avg = current_df.groupby('Hour')[['mRetweets', 'mLikes']].mean()

# Plot the average retweets and likes by hour
plt.figure(figsize=(12, 6))
hourly_avg.plot(kind='line', marker='o')
plt.xlabel('Hour of Day')
plt.ylabel('Average Count')
plt.title('Average Retweets and Likes by Hour of Day')
plt.grid(True)
plt.show()

current_df['TweetLength'] = current_df['text'].str.len()

# Plot the distribution of tweet lengths
plt.figure(figsize=(12, 6))
current_df['TweetLength'].plot(kind='hist', bins=50, color='skyblue')
plt.xlabel('Tweet Length (characters)')
plt.ylabel('Frequency')
plt.title('Distribution of Tweet Lengths')
plt.grid(True)
plt.show()

current_df['Date'] = current_df['CreatedAt'].dt.date

# Group by date and count tweets with and without URLs
url_counts = current_df.groupby(['Date', 'mHasURL']).size().unstack(fill_value=0)

# Plot the tweet counts with and without URLs over time
plt.figure(figsize=(12, 6))
url_counts.plot(kind='line')
plt.xlabel('Date')
plt.ylabel('Number of Tweets')
plt.title('Tweets with URLs Over Time')
plt.legend(['No URL', 'Has URL'])
plt.grid(True)
plt.show()

from wordcloud import WordCloud

# Combine all text from tweets
all_text = ' '.join(current_df['text'].dropna())

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Plot the word cloud
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Common Words in Tweets')
plt.show()

# Plot the relationship between retweets and likes
plt.figure(figsize=(12, 6))
plt.scatter(current_df['mRetweets'], current_df['mLikes'], alpha=0.5, color='skyblue')
plt.xlabel('Number of Retweets')
plt.ylabel('Number of Likes')
plt.title('Correlation between Retweets and Likes')
plt.grid(True)
plt.show()

import seaborn as sns

# Create a new column for the day of the week
current_df['DayOfWeek'] = current_df['CreatedAt'].dt.day_name()

# Create a heatmap of tweet activity by hour and day of the week
hour_day_pivot = current_df.pivot_table(index='Hour', columns='DayOfWeek', values='Id_b36', aggfunc='count').fillna(0)
plt.figure(figsize=(12, 6))
sns.heatmap(hour_day_pivot, cmap='Blues')
plt.xlabel('Day of Week')
plt.ylabel('Hour of Day')
plt.title('Hourly Tweet Activity by Day of Week')
plt.show()

# Group by label and calculate average engagement metrics
label_engagement = current_df.groupby('LABEL')[['mRetweets', 'mLikes', 'mReplies']].mean()

# Plot the average engagement metrics by label
plt.figure(figsize=(12, 6))
label_engagement.plot(kind='bar')
plt.xlabel('Label')
plt.ylabel('Average Count')
plt.title('Average Engagement by Label')
plt.grid(True)
plt.show()

# Function to get sentiment score
def get_sentiment_score(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

# Apply sentiment analysis
current_df['SentimentScore'] = current_df['text'].apply(get_sentiment_score)

# Group by date and calculate the average sentiment score
sentiment_over_time = current_df.groupby(current_df['CreatedAt'].dt.date)['SentimentScore'].mean()

# Plot the average sentiment score over time
plt.figure(figsize=(12, 6))
sentiment_over_time.plot(kind='line')
plt.xlabel('Date')
plt.ylabel('Average Sentiment Score')
plt.title('Tweet Sentiment Over Time')
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Convert 'mRetweets' column to numeric type
current_df['mRetweets'] = pd.to_numeric(current_df['mRetweets'], errors='coerce')

# Sort the DataFrame by likes and retweets to get the top 15 tweets
top_likes = current_df.nlargest(15, 'mLikes')
top_retweets = current_df.nlargest(15, 'mRetweets')

# Create subplots with 1 row and 2 columns
fig, axs = plt.subplots(1, 2, figsize=(16, 6))

# Plot the top 15 tweets with the most likes
axs[0].barh(range(len(top_likes)), top_likes['mLikes'], color='skyblue')
axs[0].set_yticks(range(len(top_likes)))
axs[0].set_yticklabels(top_likes['Id_b36'], fontsize=10)  # Use Id_b36 column for tweet number
axs[0].set_xlabel('Number of Likes')
axs[0].set_title('Top 15 Tweets with Most Likes')

# Plot the top 15 tweets with the most retweets
axs[1].barh(range(len(top_retweets)), top_retweets['mRetweets'], color='lightgreen')
axs[1].set_yticks(range(len(top_retweets)))
axs[1].set_yticklabels(top_retweets['Id_b36'], fontsize=10)  # Use Id_b36 column for tweet number
axs[1].set_xlabel('Number of Retweets')
axs[1].set_title('Top 15 Tweets with Most Retweets')

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Convert 'mRetweets' column to numeric type
current_df['mRetweets'] = pd.to_numeric(current_df['mRetweets'], errors='coerce')

# Sort the DataFrame by likes and retweets to get the top 15 tweets
top_likes = current_df.nlargest(15, 'mLikes')
top_retweets = current_df.nlargest(15, 'mRetweets')

# Define a color mapping dictionary for the LABEL values
color_mapping = {1: 'blue', 2: 'red', 3: 'green'}

# Create subplots with 1 row and 2 columns
fig, axs = plt.subplots(1, 2, figsize=(16, 6))

# Plot the top 15 tweets with the most likes
for idx, tweet_id in enumerate(top_likes['Id_b36']):
    label = current_df.loc[current_df['Id_b36'] == tweet_id, 'LABEL'].values[0]
    color = color_mapping.get(label, 'black')  # Default to black if label not found
    axs[0].barh(idx, top_likes.loc[top_likes['Id_b36'] == tweet_id, 'mLikes'], color=color)

axs[0].set_yticks(range(len(top_likes)))
axs[0].set_yticklabels(top_likes['Id_b36'], fontsize=10)
axs[0].set_xlabel('Number of Likes')
axs[0].set_title('Top 15 Tweets with Most Likes')

# Plot the top 15 tweets with the most retweets
for idx, tweet_id in enumerate(top_retweets['Id_b36']):
    label = current_df.loc[current_df['Id_b36'] == tweet_id, 'LABEL'].values[0]
    color = color_mapping.get(label, 'black')  # Default to black if label not found
    axs[1].barh(idx, top_retweets.loc[top_retweets['Id_b36'] == tweet_id, 'mRetweets'], color=color)

axs[1].set_yticks(range(len(top_retweets)))
axs[1].set_yticklabels(top_retweets['Id_b36'], fontsize=10)
axs[1].set_xlabel('Number of Retweets')
axs[1].set_title('Top 15 Tweets with Most Retweets')

plt.tight_layout()
plt.show()

current_df

tweet_text = current_df.loc[current_df['Id_b36'] == "BY8UFUX69S02", 'text'].values[0]
label_value = current_df.loc[current_df['Id_b36'] == "BY8UFUX69S02", 'LABEL'].values[0]

print("Tweet Text:", tweet_text)
print("Label Value:", label_value)







